import{j as e,__tla as o}from"./index-CWwRKoNd.js";let i,s=Promise.all([(()=>{try{return o}catch{}})()]).then(async()=>{function a(t){const n={a:"a",h2:"h2",h3:"h3",img:"img",li:"li",p:"p",ul:"ul",...t.components};return e.jsxs(e.Fragment,{children:[e.jsx(n.h2,{children:"Local Embedding Models"}),`
`,e.jsx(n.p,{children:"The application supports local embedding models for data representation and analysis. These models can be run directly on your machine, providing a fast and efficient way to process data without relying on external servers."}),`
`,e.jsx(n.p,{children:"Embedding models represent data in a lower-dimensional space, making it easier to analyze and visualize. They are commonly used in machine learning and data analysis applications."}),`
`,e.jsx(n.p,{children:e.jsx(n.img,{src:"./docs/nomic-ai.svg",alt:"Nomic"})}),`
`,e.jsx(n.p,{children:"Modern AI models are trained on internet sized datasets, run on supercomputers, and enable content production on an unprecedented scale. At Nomic, we build tools that enable everyone to interact with AI scale datasets and run AI models on consumer computers."}),`
`,e.jsx(n.h3,{children:"1. nomic-ai/nomic-embed-text-v1.5"}),`
`,e.jsx(n.p,{children:e.jsx(n.img,{src:"./docs/nomic_beats_ada_wide.webp",alt:"Nomic Embed Text"})}),`
`,e.jsx(n.p,{children:"text embedding model with a 8192 context-length that outperforms OpenAI Ada-002 and text-embedding-3-small on both short and long context tasks. We release the model weights and training code under an Apache-2 license, as well as the curated data we used to train the model. We also release a detailed technical report."}),`
`,e.jsx(n.p,{children:"Nomic Embed is in general availability for production workloads through the Nomic Atlas Embedding API with 1M free tokens included and is enterprise-ready via our fully secure and compliant Nomic Atlas Enterprise offering."}),`
`,e.jsx(n.p,{children:"Text embeddings are an integral component of modern NLP applications powering retrieval-augmented-generation (RAG) for LLMs and semantic search. They encode semantic information about sentences or documents into low-dimensional vectors that are then used in downstream applications, such as clustering for data visualization, classification, and information retrieval. Currently, the most popular long-context text embedding model is OpenAI's text-embedding-ada-002, which supports a context length of 8192. Unfortunately Ada is closed source and it's training data is not auditable."}),`
`,e.jsx(n.p,{children:"Top performing open source long-context text embedding models such E5-Mistral and jina-embeddings-v2-base-en are either not practical for general-purpose use due to model size or fail to exceed the performance of their OpenAI counterparts."}),`
`,e.jsx(n.h2,{children:"Additional Resources"}),`
`,e.jsxs(n.ul,{children:[`
`,e.jsx(n.li,{children:e.jsx(n.a,{href:"./#/document/get-started",title:"<_self>",children:"Get started Example"})}),`
`]}),`
`,e.jsx("br",{}),`
`,e.jsx("br",{}),`
`,e.jsx("br",{}),`
`,e.jsx("br",{}),`
`,e.jsx("br",{})]})}i=function(t={}){const{wrapper:n}=t.components||{};return n?e.jsx(n,{...t,children:e.jsx(a,{...t})}):a(t)}});export{s as __tla,i as default};
